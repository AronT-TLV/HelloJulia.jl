{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Lightning tour of MLJ"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "*For a more elementary introduction to MLJ, see [Getting\n",
    "Started](https://alan-turing-institute.github.io/MLJ.jl/dev/getting_started/).*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Inspect Julia version:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "v\"1.6.3\""
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "cell_type": "code",
   "source": [
    "VERSION"
   ],
   "metadata": {},
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following instantiates a package environment."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The package environment has been created using **Julia 1.6** and may not\n",
    "instantiate properly for other Julia versions."
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Activating environment at `~/GoogleDrive/Julia/HelloJulia/demos/machine_learning_in_julia/env/Project.toml`\n",
      "Precompiling project...\n",
      "\u001b[33m  ✓ \u001b[39m\u001b[90mMissings\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mMLJOpenML\u001b[39m\n",
      "\u001b[33m  ✓ \u001b[39mLiterate\n",
      "\u001b[33m  ✓ \u001b[39m\u001b[90mTables\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mPersistenceDiagramsBase\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mCategoricalArrays\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mStatsBase\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mStructArrays\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mLatinHypercubeSampling\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mLossFunctions\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mPrettyTables\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mScientificTypes\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mDistributions\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mGeometryBasics\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mNetworkLayout\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mMLJBase\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mMLJEnsembles\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39mMLJIteration\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mMLJTuning\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mMLJSerialization\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39mEvoTrees\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mMLJModels\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39mMLJ\n",
      "  23 dependencies successfully precompiled in 32 seconds (79 already precompiled)\n",
      "  \u001b[33m3\u001b[39m dependencies precompiled but different versions are currently loaded. Restart julia to access the new versions\n"
     ]
    }
   ],
   "cell_type": "code",
   "source": [
    "using Pkg\n",
    "Pkg.activate(\"env\")\n",
    "Pkg.instantiate()"
   ],
   "metadata": {},
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "In MLJ a *model* is just a container for hyper-parameters, and that's\n",
    "all. Here we will apply several kinds of model composition before\n",
    "binding the resulting \"meta-model\" to data in a *machine* for\n",
    "evaluation, using cross-validation."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Loading and instantiating a gradient tree-boosting model:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Info: Precompiling MLJ [add582a8-e3ab-11e8-2d5e-e98b27df1bc7]\n",
      "[ Info: For silent loading, specify `verbosity=0`. \n",
      "import EvoTrees[ Info: Precompiling EvoTrees [f6006082-12f8-11e9-0c9c-0d5d367ab1e5]\n",
      " ✔\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "EvoTreeRegressor(\n    loss = EvoTrees.Linear(),\n    nrounds = 10,\n    λ = 0.0,\n    γ = 0.0,\n    η = 0.1,\n    max_depth = 2,\n    min_weight = 1.0,\n    rowsample = 1.0,\n    colsample = 1.0,\n    nbins = 64,\n    α = 0.5,\n    metric = :mse,\n    rng = MersenneTwister(444),\n    device = \"cpu\") @677"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "cell_type": "code",
   "source": [
    "using MLJ\n",
    "MLJ.color_off()\n",
    "\n",
    "Booster = @load EvoTreeRegressor # loads code defining a model type\n",
    "booster = Booster(max_depth=2)   # specify hyper-parameter at construction"
   ],
   "metadata": {},
   "execution_count": 3
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "EvoTreeRegressor(\n    loss = EvoTrees.Linear(),\n    nrounds = 50,\n    λ = 0.0,\n    γ = 0.0,\n    η = 0.1,\n    max_depth = 2,\n    min_weight = 1.0,\n    rowsample = 1.0,\n    colsample = 1.0,\n    nbins = 64,\n    α = 0.5,\n    metric = :mse,\n    rng = MersenneTwister(444),\n    device = \"cpu\") @677"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "cell_type": "code",
   "source": [
    "booster.nrounds=50               # or mutate post facto\n",
    "booster"
   ],
   "metadata": {},
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "This model is an example of an iterative model. As is stands, the\n",
    "number of iterations `nrounds` is fixed."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Composition 1: Wrapping the model to make it \"self-iterating\""
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's create a new model that automatically learns the number of iterations,\n",
    "using the `NumberSinceBest(3)` criterion, as applied to an\n",
    "out-of-sample `l1` loss:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "DeterministicIteratedModel(\n    model = EvoTreeRegressor(\n            loss = EvoTrees.Linear(),\n            nrounds = 50,\n            λ = 0.0,\n            γ = 0.0,\n            η = 0.1,\n            max_depth = 2,\n            min_weight = 1.0,\n            rowsample = 1.0,\n            colsample = 1.0,\n            nbins = 64,\n            α = 0.5,\n            metric = :mse,\n            rng = MersenneTwister(444),\n            device = \"cpu\"),\n    controls = Any[IterationControl.Step(2), EarlyStopping.NumberSinceBest(3), EarlyStopping.NumberLimit(300)],\n    resampling = Holdout(\n            fraction_train = 0.8,\n            shuffle = false,\n            rng = Random._GLOBAL_RNG()),\n    measure = LPLoss(\n            p = 1),\n    weights = nothing,\n    class_weights = nothing,\n    operation = MLJModelInterface.predict,\n    retrain = true,\n    check_measure = true,\n    iteration_parameter = nothing,\n    cache = true) @232"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "cell_type": "code",
   "source": [
    "using MLJIteration\n",
    "iterated_booster = IteratedModel(model=booster,\n",
    "                                 resampling=Holdout(fraction_train=0.8),\n",
    "                                 controls=[Step(2), NumberSinceBest(3), NumberLimit(300)],\n",
    "                                 measure=l1,\n",
    "                                 retrain=true)"
   ],
   "metadata": {},
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Composition 2: Preprocess the input features"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Combining the model with categorical feature encoding:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Pipeline260(\n    continuous_encoder = ContinuousEncoder(\n            drop_last = false,\n            one_hot_ordered_factors = false),\n    deterministic_iterated_model = DeterministicIteratedModel(\n            model = EvoTreeRegressor{Float64,…} @677,\n            controls = Any[IterationControl.Step(2), EarlyStopping.NumberSinceBest(3), EarlyStopping.NumberLimit(300)],\n            resampling = Holdout @158,\n            measure = LPLoss{Int64} @985,\n            weights = nothing,\n            class_weights = nothing,\n            operation = MLJModelInterface.predict,\n            retrain = true,\n            check_measure = true,\n            iteration_parameter = nothing,\n            cache = true)) @457"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "cell_type": "code",
   "source": [
    "pipe = @pipeline ContinuousEncoder iterated_booster"
   ],
   "metadata": {},
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Composition 3: Wrapping the model to make it \"self-tuning\""
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, we define a hyper-parameter range for optimization of a\n",
    "(nested) hyper-parameter:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "typename(MLJBase.NumericRange)(Int64, :(deterministic_iterated_model.model.max_depth), ... )"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "cell_type": "code",
   "source": [
    "max_depth_range = range(pipe,\n",
    "                        :(deterministic_iterated_model.model.max_depth),\n",
    "                        lower = 1,\n",
    "                        upper = 10)"
   ],
   "metadata": {},
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can wrap the pipeline model in an optimization strategy to make\n",
    "it \"self-tuning\":"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "DeterministicTunedModel(\n    model = Pipeline260(\n            continuous_encoder = ContinuousEncoder @631,\n            deterministic_iterated_model = DeterministicIteratedModel{EvoTreeRegressor{Float64,…}} @232),\n    tuning = RandomSearch(\n            bounded = Distributions.Uniform,\n            positive_unbounded = Distributions.Gamma,\n            other = Distributions.Normal,\n            rng = Random._GLOBAL_RNG()),\n    resampling = CV(\n            nfolds = 3,\n            shuffle = true,\n            rng = MersenneTwister(456)),\n    measure = LPLoss(\n            p = 1),\n    weights = nothing,\n    operation = MLJModelInterface.predict,\n    range = NumericRange(\n            field = :(deterministic_iterated_model.model.max_depth),\n            lower = 1,\n            upper = 10,\n            origin = 5.5,\n            unit = 4.5,\n            scale = :linear),\n    selection_heuristic = MLJTuning.NaiveSelection(nothing),\n    train_best = true,\n    repeats = 1,\n    n = 50,\n    acceleration = ComputationalResources.CPUThreads{Int64}(5),\n    acceleration_resampling = ComputationalResources.CPU1{Nothing}(nothing),\n    check_measure = true,\n    cache = true) @384"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "cell_type": "code",
   "source": [
    "self_tuning_pipe = TunedModel(model=pipe,\n",
    "                              tuning=RandomSearch(),\n",
    "                              ranges = max_depth_range,\n",
    "                              resampling=CV(nfolds=3, rng=456),\n",
    "                              measure=l1,\n",
    "                              acceleration=CPUThreads(),\n",
    "                              n=50)"
   ],
   "metadata": {},
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Binding to data and evaluating performance"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Creating some synthetic features and labels:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "X, y = make_regression();"
   ],
   "metadata": {},
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "Binding the \"self-tuning\" pipeline model to data in a *machine* (which\n",
    "will additionally store *learned* parameters):"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Machine{DeterministicTunedModel{RandomSearch,…},…} @483 trained 0 times; caches data\n  args: \n    1:\tSource @892 ⏎ `ScientificTypesBase.Table{AbstractVector{ScientificTypesBase.Continuous}}`\n    2:\tSource @095 ⏎ `AbstractVector{ScientificTypesBase.Continuous}`\n"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "cell_type": "code",
   "source": [
    "mach = machine(self_tuning_pipe, X, y)"
   ],
   "metadata": {},
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "Evaluating the \"self-tuning\" pipeline model's performance using 5-fold\n",
    "cross-validation (implies multiple layers of nested resampling):"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Info: Performing evaluations using 5 threads.\n",
      "\rEvaluating over 5 folds:  40%[==========>              ]  ETA: 0:01:41\u001b[K\rEvaluating over 5 folds: 100%[=========================] Time: 0:01:14\u001b[K\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "┌────────────────────┬───────────────┬─────────────────────────────────────┐\n│\u001b[22m _.measure          \u001b[0m│\u001b[22m _.measurement \u001b[0m│\u001b[22m _.per_fold                          \u001b[0m│\n├────────────────────┼───────────────┼─────────────────────────────────────┤\n│ LPLoss{Int64} @985 │ 0.207         │ [0.186, 0.177, 0.139, 0.218, 0.316] │\n│ LPLoss{Int64} @787 │ 0.0929        │ [0.062, 0.0618, 0.03, 0.109, 0.202] │\n└────────────────────┴───────────────┴─────────────────────────────────────┘\n_.per_observation = [[[0.167, 0.0809, ..., 0.0223], [0.461, 0.0773, ..., 0.0616], [0.13, 0.029, ..., 0.0315], [0.21, 0.0984, ..., 0.255], [0.338, 0.223, ..., 0.0829]], [[0.0277, 0.00655, ..., 0.000498], [0.212, 0.00597, ..., 0.00379], [0.0169, 0.000841, ..., 0.000992], [0.044, 0.00968, ..., 0.0651], [0.115, 0.0499, ..., 0.00687]]]\n_.fitted_params_per_fold = [ … ]\n_.report_per_fold = [ … ]\n_.train_test_rows = [ … ]\n"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "cell_type": "code",
   "source": [
    "evaluate!(mach,\n",
    "          measures=[l1, l2],\n",
    "          resampling=CV(nfolds=5, rng=123),\n",
    "          acceleration=CPUThreads())"
   ],
   "metadata": {},
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.3"
  },
  "kernelspec": {
   "name": "julia-1.6",
   "display_name": "Julia 1.6.3",
   "language": "julia"
  }
 },
 "nbformat": 4
}
