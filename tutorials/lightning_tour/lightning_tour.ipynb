{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Lightning tour of MLJ"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "*For a more elementary introduction to MLJ, see [Getting\n",
    "Started](https://alan-turing-institute.github.io/MLJ.jl/dev/getting_started/).*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Install required packages (beginners needn't worry about\n",
    "understanding this cell):"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Activating environment at `~/Google Drive/Julia/HelloJulia/tutorials/lightning_tour/src/Project.toml`\n"
     ]
    }
   ],
   "cell_type": "code",
   "source": [
    "using Pkg\n",
    "DIR = joinpath(@__DIR__, \"src\")\n",
    "Pkg.activate(DIR)\n",
    "include(joinpath(DIR, \"instantiate.jl\"))"
   ],
   "metadata": {},
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "In MLJ a *model* is just a container for hyper-parameters, and that's\n",
    "all. Here we will apply several kinds of model composition before\n",
    "binding the resulting \"meta-model\" to data in a *machine* for\n",
    "evaluation, using cross-validation."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Loading and instantiating a gradient tree-boosting model:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌ Info: For silent loading, specify `verbosity=0`. \n",
      "└ @ Main.##405 /Users/anthony/.julia/packages/MLJModels/5itei/src/loading.jl:168\n",
      "import EvoTrees ✔\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "EvoTreeRegressor(\n    loss = EvoTrees.Linear(),\n    nrounds = 10,\n    λ = 0.0,\n    γ = 0.0,\n    η = 0.1,\n    max_depth = 2,\n    min_weight = 1.0,\n    rowsample = 1.0,\n    colsample = 1.0,\n    nbins = 64,\n    α = 0.5,\n    metric = :mse,\n    rng = MersenneTwister(444),\n    device = \"cpu\") @344"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "cell_type": "code",
   "source": [
    "using MLJ\n",
    "MLJ.color_off()\n",
    "\n",
    "Booster = @load EvoTreeRegressor # loads code defining a model type\n",
    "booster = Booster(max_depth=2)   # specify hyper-parameter at construction"
   ],
   "metadata": {},
   "execution_count": 2
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "EvoTreeRegressor(\n    loss = EvoTrees.Linear(),\n    nrounds = 50,\n    λ = 0.0,\n    γ = 0.0,\n    η = 0.1,\n    max_depth = 2,\n    min_weight = 1.0,\n    rowsample = 1.0,\n    colsample = 1.0,\n    nbins = 64,\n    α = 0.5,\n    metric = :mse,\n    rng = MersenneTwister(444),\n    device = \"cpu\") @344"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "cell_type": "code",
   "source": [
    "booster.nrounds=50               # or mutate post facto\n",
    "booster"
   ],
   "metadata": {},
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "This model is an example of an iterative model. As is stands, the\n",
    "number of iterations `nrounds` is fixed."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Composition 1: Wrapping the model to make it \"self-iterating\""
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's create a new model that automatically learns the number of iterations,\n",
    "using the `NumberSinceBest(3)` criterion, as applied to an\n",
    "out-of-sample `l1` loss:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "DeterministicIteratedModel(\n    model = EvoTreeRegressor(\n            loss = EvoTrees.Linear(),\n            nrounds = 50,\n            λ = 0.0,\n            γ = 0.0,\n            η = 0.1,\n            max_depth = 2,\n            min_weight = 1.0,\n            rowsample = 1.0,\n            colsample = 1.0,\n            nbins = 64,\n            α = 0.5,\n            metric = :mse,\n            rng = MersenneTwister(444),\n            device = \"cpu\"),\n    controls = Any[Step(2), NumberSinceBest(3), NumberLimit(300)],\n    resampling = Holdout(\n            fraction_train = 0.8,\n            shuffle = false,\n            rng = Random._GLOBAL_RNG()),\n    measure = LPLoss(\n            p = 1),\n    weights = nothing,\n    class_weights = nothing,\n    operation = MLJModelInterface.predict,\n    retrain = true,\n    check_measure = true,\n    iteration_parameter = nothing,\n    cache = true) @399"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "cell_type": "code",
   "source": [
    "using MLJIteration\n",
    "iterated_booster = IteratedModel(model=booster,\n",
    "                                 resampling=Holdout(fraction_train=0.8),\n",
    "                                 controls=[Step(2), NumberSinceBest(3), NumberLimit(300)],\n",
    "                                 measure=l1,\n",
    "                                 retrain=true)"
   ],
   "metadata": {},
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Composition 2: Preprocess the input features"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Combining the model with categorical feature encoding:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Pipeline408(\n    continuous_encoder = ContinuousEncoder(\n            drop_last = false,\n            one_hot_ordered_factors = false),\n    deterministic_iterated_model = DeterministicIteratedModel(\n            model = EvoTreeRegressor{Float64,…} @344,\n            controls = Any[Step(2), NumberSinceBest(3), NumberLimit(300)],\n            resampling = Holdout @414,\n            measure = LPLoss{Int64} @489,\n            weights = nothing,\n            class_weights = nothing,\n            operation = MLJModelInterface.predict,\n            retrain = true,\n            check_measure = true,\n            iteration_parameter = nothing,\n            cache = true)) @230"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "cell_type": "code",
   "source": [
    "pipe = @pipeline ContinuousEncoder iterated_booster"
   ],
   "metadata": {},
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Composition 3: Wrapping the model to make it \"self-tuning\""
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, we define a hyper-parameter range for optimization of a\n",
    "(nested) hyper-parameter:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "typename(MLJBase.NumericRange)(Int64, :(deterministic_iterated_model.model.max_depth), ... )"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "cell_type": "code",
   "source": [
    "max_depth_range = range(pipe,\n",
    "                        :(deterministic_iterated_model.model.max_depth),\n",
    "                        lower = 1,\n",
    "                        upper = 10)"
   ],
   "metadata": {},
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can wrap the pipeline model in an optimization strategy to make\n",
    "it \"self-tuning\":"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "DeterministicTunedModel(\n    model = Pipeline408(\n            continuous_encoder = ContinuousEncoder @733,\n            deterministic_iterated_model = DeterministicIteratedModel{EvoTreeRegressor{Float64,…}} @399),\n    tuning = RandomSearch(\n            bounded = Distributions.Uniform,\n            positive_unbounded = Distributions.Gamma,\n            other = Distributions.Normal,\n            rng = Random._GLOBAL_RNG()),\n    resampling = CV(\n            nfolds = 3,\n            shuffle = true,\n            rng = MersenneTwister(456)),\n    measure = LPLoss(\n            p = 1),\n    weights = nothing,\n    operation = MLJModelInterface.predict,\n    range = NumericRange(\n            field = :(deterministic_iterated_model.model.max_depth),\n            lower = 1,\n            upper = 10,\n            origin = 5.5,\n            unit = 4.5,\n            scale = :linear),\n    selection_heuristic = MLJTuning.NaiveSelection(nothing),\n    train_best = true,\n    repeats = 1,\n    n = 50,\n    acceleration = CPUThreads{Int64}(5),\n    acceleration_resampling = CPU1{Nothing}(nothing),\n    check_measure = true,\n    cache = true) @884"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "cell_type": "code",
   "source": [
    "self_tuning_pipe = TunedModel(model=pipe,\n",
    "                              tuning=RandomSearch(),\n",
    "                              ranges = max_depth_range,\n",
    "                              resampling=CV(nfolds=3, rng=456),\n",
    "                              measure=l1,\n",
    "                              acceleration=CPUThreads(),\n",
    "                              n=50)"
   ],
   "metadata": {},
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Binding to data and evaluating performance"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Loading a selection of features and labels from the Ames\n",
    "House Price dataset:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "X, y = make_regression();"
   ],
   "metadata": {},
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "Binding the \"self-tuning\" pipeline model to data in a *machine* (which\n",
    "will additionally store *learned* parameters):"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Machine{DeterministicTunedModel{RandomSearch,…},…} @614 trained 0 times; caches data\n  args: \n    1:\tSource @070 ⏎ `Table{AbstractVector{Continuous}}`\n    2:\tSource @412 ⏎ `AbstractVector{Continuous}`\n"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "cell_type": "code",
   "source": [
    "mach = machine(self_tuning_pipe, X, y)"
   ],
   "metadata": {},
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "Evaluating the \"self-tuning\" pipeline model's performance using 5-fold\n",
    "cross-validation (implies multiple layers of nested resampling):"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┌ Info: Performing evaluations using 5 threads.\n",
      "└ @ MLJBase /Users/anthony/.julia/packages/MLJBase/rN59G/src/resampling.jl:998\n",
      "\rEvaluating over 5 folds:  40%[==========>              ]  ETA: 0:00:17\u001b[K\rEvaluating over 5 folds:  60%[===============>         ]  ETA: 0:00:08\u001b[K\rEvaluating over 5 folds:  80%[====================>    ]  ETA: 0:00:03\u001b[K\rEvaluating over 5 folds: 100%[=========================] Time: 0:00:22\u001b[K\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "┌────────────────────┬───────────────┬──────────────────────────────────────┐\n│\u001b[22m _.measure          \u001b[0m│\u001b[22m _.measurement \u001b[0m│\u001b[22m _.per_fold                           \u001b[0m│\n├────────────────────┼───────────────┼──────────────────────────────────────┤\n│ LPLoss{Int64} @489 │ 0.198         │ [0.251, 0.187, 0.179, 0.198, 0.176]  │\n│ LPLoss{Int64} @595 │ 0.0725        │ [0.1, 0.116, 0.0452, 0.0544, 0.0471] │\n└────────────────────┴───────────────┴──────────────────────────────────────┘\n_.per_observation = [[[0.313, 0.102, ..., 0.606], [0.0406, 0.373, ..., 0.0288], [0.152, 0.131, ..., 0.41], [0.303, 0.378, ..., 0.069], [0.34, 0.158, ..., 0.0683]], [[0.098, 0.0105, ..., 0.368], [0.00165, 0.139, ..., 0.000829], [0.023, 0.0171, ..., 0.168], [0.0916, 0.143, ..., 0.00476], [0.116, 0.0249, ..., 0.00467]]]\n_.fitted_params_per_fold = [ … ]\n_.report_per_fold = [ … ]\n_.train_test_rows = [ … ]\n"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "cell_type": "code",
   "source": [
    "evaluate!(mach,\n",
    "          measures=[l1, l2],\n",
    "          resampling=CV(nfolds=5, rng=123),\n",
    "          acceleration=CPUThreads())"
   ],
   "metadata": {},
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.3"
  },
  "kernelspec": {
   "name": "julia-1.6",
   "display_name": "Julia 1.6.3",
   "language": "julia"
  }
 },
 "nbformat": 4
}
