{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lightning tour of MLJ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*For a more elementary introduction to MLJ, see [Getting\n",
    "Started](https://alan-turing-institute.github.io/MLJ.jl/dev/getting_started/).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note.** Be sure this file has not been separated from the\n",
    "accompanying Project.toml and Manifest.toml files, which should not\n",
    "should be altered unless you know what you are doing. Using them,\n",
    "the following code block instantiates a julia environment with a tested\n",
    "bundle of packages known to work with the rest of the script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m environment at `~/Google Drive/Julia/HelloJulia/tutorials/lightning_tour/Project.toml`\n",
      "\u001b[32m\u001b[1mPrecompiling\u001b[22m\u001b[39m project...\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mDataAPI\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mReexport\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mScientificTypesBase\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mBFloat16s\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mRecipesBase\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mLogExpFunctions\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mCompat\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mMissings\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mStructTypes\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mStatisticalTraits\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mDistances\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mFillArrays\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mEarCut_jll\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mTables\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mMLJModelInterface\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mLLVMExtra_jll\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mChainRulesCore\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mPersistenceDiagramsBase\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mHTTP\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mGPUArrays\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mDataStructures\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mTimeZones\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mStructArrays\u001b[39m\n",
      "\u001b[33m  ✓ \u001b[39m\u001b[90mParsers\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mSortingAlgorithms\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mQuadGK\u001b[39m\n",
      "\u001b[33m  ✓ \u001b[39m\u001b[90mJSON\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39mLiterate\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mMLJOpenML\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mLLVM\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mStatsBase\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mCategoricalArrays\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mSpecialFunctions\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mLatinHypercubeSampling\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mPrettyTables\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mMemento\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mLossFunctions\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mStatsFuns\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mScientificTypes\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mJLSO\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mGPUCompiler\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mGeometryBasics\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mNetworkLayout\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mDistributions\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mMLJBase\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39mMLJIteration\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mMLJEnsembles\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mMLJTuning\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mMLJSerialization\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mMLJModels\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39mMLJ\n",
      "\u001b[32m  ✓ \u001b[39m\u001b[90mCUDA\u001b[39m\n",
      "\u001b[32m  ✓ \u001b[39mEvoTrees\n",
      "  53 dependencies successfully precompiled in 45 seconds (50 already precompiled)\n",
      "  \u001b[33m2\u001b[39m dependencies precompiled but different versions are currently loaded. Restart julia to access the new versions\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "Pkg.activate(@__DIR__)\n",
    "Pkg.instantiate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming Julia 1.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In MLJ a *model* is just a container for hyper-parameters, and that's\n",
    "all. Here we will apply several kinds of model composition before\n",
    "binding the resulting \"meta-model\" to data in a *machine* for\n",
    "evaluation, using cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading and instantiating a gradient tree-boosting model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Precompiling MLJ [add582a8-e3ab-11e8-2d5e-e98b27df1bc7]\n",
      "└ @ Base loading.jl:1342\n",
      "┌ Info: For silent loading, specify `verbosity=0`. \n",
      "└ @ Main /Users/anthony/.julia/packages/MLJModels/5itei/src/loading.jl:168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import EvoTrees"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Precompiling EvoTrees [f6006082-12f8-11e9-0c9c-0d5d367ab1e5]\n",
      "└ @ Base loading.jl:1342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✔\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EvoTreeRegressor(\n",
       "    loss = EvoTrees.Linear(),\n",
       "    nrounds = 10,\n",
       "    λ = 0.0,\n",
       "    γ = 0.0,\n",
       "    η = 0.1,\n",
       "    max_depth = 2,\n",
       "    min_weight = 1.0,\n",
       "    rowsample = 1.0,\n",
       "    colsample = 1.0,\n",
       "    nbins = 64,\n",
       "    α = 0.5,\n",
       "    metric = :mse,\n",
       "    rng = MersenneTwister(444),\n",
       "    device = \"cpu\") @284"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using MLJ\n",
    "MLJ.color_off()\n",
    "\n",
    "Booster = @load EvoTreeRegressor # loads code defining a model type\n",
    "booster = Booster(max_depth=2)   # specify hyper-parameter at construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvoTreeRegressor(\n",
       "    loss = EvoTrees.Linear(),\n",
       "    nrounds = 50,\n",
       "    λ = 0.0,\n",
       "    γ = 0.0,\n",
       "    η = 0.1,\n",
       "    max_depth = 2,\n",
       "    min_weight = 1.0,\n",
       "    rowsample = 1.0,\n",
       "    colsample = 1.0,\n",
       "    nbins = 64,\n",
       "    α = 0.5,\n",
       "    metric = :mse,\n",
       "    rng = MersenneTwister(444),\n",
       "    device = \"cpu\") @522"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "booster.nrounds=50               # or mutate post facto\n",
    "booster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is an example of an iterative model. As is stands, the\n",
    "number of iterations `nrounds` is fixed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Composition 1: Wrapping the model to make it \"self-iterating\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a new model that automatically learns the number of iterations,\n",
    "using the `NumberSinceBest(3)` criterion, as applied to an\n",
    "out-of-sample `l1` loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeterministicIteratedModel(\n",
       "    model = EvoTreeRegressor(\n",
       "            loss = EvoTrees.Linear(),\n",
       "            nrounds = 50,\n",
       "            λ = 0.0,\n",
       "            γ = 0.0,\n",
       "            η = 0.1,\n",
       "            max_depth = 2,\n",
       "            min_weight = 1.0,\n",
       "            rowsample = 1.0,\n",
       "            colsample = 1.0,\n",
       "            nbins = 64,\n",
       "            α = 0.5,\n",
       "            metric = :mse,\n",
       "            rng = MersenneTwister(444),\n",
       "            device = \"cpu\"),\n",
       "    controls = Any[Step(2), NumberSinceBest(3), NumberLimit(300)],\n",
       "    resampling = Holdout(\n",
       "            fraction_train = 0.8,\n",
       "            shuffle = false,\n",
       "            rng = Random._GLOBAL_RNG()),\n",
       "    measure = LPLoss(\n",
       "            p = 1),\n",
       "    weights = nothing,\n",
       "    class_weights = nothing,\n",
       "    operation = MLJModelInterface.predict,\n",
       "    retrain = true,\n",
       "    check_measure = true,\n",
       "    iteration_parameter = nothing,\n",
       "    cache = true) @630"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using MLJIteration\n",
    "iterated_booster = IteratedModel(model=booster,\n",
    "                                 resampling=Holdout(fraction_train=0.8),\n",
    "                                 controls=[Step(2), NumberSinceBest(3), NumberLimit(300)],\n",
    "                                 measure=l1,\n",
    "                                 retrain=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Composition 2: Preprocess the input features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining the model with categorical feature encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline290(\n",
       "    continuous_encoder = ContinuousEncoder(\n",
       "            drop_last = false,\n",
       "            one_hot_ordered_factors = false),\n",
       "    deterministic_iterated_model = DeterministicIteratedModel(\n",
       "            model = EvoTreeRegressor{Float64,…} @522,\n",
       "            controls = Any[Step(2), NumberSinceBest(3), NumberLimit(300)],\n",
       "            resampling = Holdout @258,\n",
       "            measure = LPLoss{Int64} @628,\n",
       "            weights = nothing,\n",
       "            class_weights = nothing,\n",
       "            operation = MLJModelInterface.predict,\n",
       "            retrain = true,\n",
       "            check_measure = true,\n",
       "            iteration_parameter = nothing,\n",
       "            cache = true)) @585"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = @pipeline ContinuousEncoder iterated_booster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Composition 3: Wrapping the model to make it \"self-tuning\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define a hyper-parameter range for optimization of a\n",
    "(nested) hyper-parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "typename(MLJBase.NumericRange)(Int64, :(deterministic_iterated_model.model.max_depth), ... )"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_depth_range = range(pipe,\n",
    "                        :(deterministic_iterated_model.model.max_depth),\n",
    "                        lower = 1,\n",
    "                        upper = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can wrap the pipeline model in an optimization strategy to make\n",
    "it \"self-tuning\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeterministicTunedModel(\n",
       "    model = Pipeline290(\n",
       "            continuous_encoder = ContinuousEncoder @294,\n",
       "            deterministic_iterated_model = DeterministicIteratedModel{EvoTreeRegressor{Float64,…}} @630),\n",
       "    tuning = RandomSearch(\n",
       "            bounded = Distributions.Uniform,\n",
       "            positive_unbounded = Distributions.Gamma,\n",
       "            other = Distributions.Normal,\n",
       "            rng = Random._GLOBAL_RNG()),\n",
       "    resampling = CV(\n",
       "            nfolds = 3,\n",
       "            shuffle = true,\n",
       "            rng = MersenneTwister(456)),\n",
       "    measure = LPLoss(\n",
       "            p = 1),\n",
       "    weights = nothing,\n",
       "    operation = MLJModelInterface.predict,\n",
       "    range = NumericRange(\n",
       "            field = :(deterministic_iterated_model.model.max_depth),\n",
       "            lower = 1,\n",
       "            upper = 10,\n",
       "            origin = 5.5,\n",
       "            unit = 4.5,\n",
       "            scale = :linear),\n",
       "    selection_heuristic = MLJTuning.NaiveSelection(nothing),\n",
       "    train_best = true,\n",
       "    repeats = 1,\n",
       "    n = 50,\n",
       "    acceleration = CPUThreads{Int64}(5),\n",
       "    acceleration_resampling = CPU1{Nothing}(nothing),\n",
       "    check_measure = true,\n",
       "    cache = true) @132"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_tuning_pipe = TunedModel(model=pipe,\n",
    "                              tuning=RandomSearch(),\n",
    "                              ranges = max_depth_range,\n",
    "                              resampling=CV(nfolds=3, rng=456),\n",
    "                              measure=l1,\n",
    "                              acceleration=CPUThreads(),\n",
    "                              n=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binding to data and evaluating performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading a selection of features and labels from the Ames\n",
    "House Price dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = @load_reduced_ames;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binding the \"self-tuning\" pipeline model to data in a *machine* (which\n",
    "will additionally store *learned* parameters):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Machine{DeterministicTunedModel{RandomSearch,…},…} @769 trained 0 times; caches data\n",
       "  args: \n",
       "    1:\tSource @557 ⏎ `Table{Union{AbstractVector{Continuous}, AbstractVector{Count}, AbstractVector{Multiclass{15}}, AbstractVector{Multiclass{25}}, AbstractVector{OrderedFactor{10}}}}`\n",
       "    2:\tSource @538 ⏎ `AbstractVector{Continuous}`\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mach = machine(self_tuning_pipe, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the \"self-tuning\" pipeline model's performance using 5-fold\n",
    "cross-validation (implies multiple layers of nested resampling):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Info: Performing evaluations using 5 threads.\n",
      "\r",
      "Evaluating over 5 folds:  40%[==========>              ]  ETA: 0:08:13\u001b[K\r",
      "Evaluating over 5 folds:  60%[===============>         ]  ETA: 0:03:44\u001b[K\r",
      "Evaluating over 5 folds:  80%[====================>    ]  ETA: 0:01:24\u001b[K\r",
      "Evaluating over 5 folds: 100%[=========================] Time: 0:06:01\u001b[K\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "┌────────────────────┬───────────────┬──────────────────────────────────────────\n",
       "│\u001b[22m _.measure          \u001b[0m│\u001b[22m _.measurement \u001b[0m│\u001b[22m _.per_fold                             \u001b[0m ⋯\n",
       "├────────────────────┼───────────────┼──────────────────────────────────────────\n",
       "│ LPLoss{Int64} @628 │ 17000.0       │ [16500.0, 16200.0, 16600.0, 16600.0, 19 ⋯\n",
       "│ LPLoss{Int64} @406 │ 6.86e8        │ [6.18e8, 6.16e8, 6.08e8, 6.21e8, 9.66e8 ⋯\n",
       "└────────────────────┴───────────────┴──────────────────────────────────────────\n",
       "\u001b[36m                                                                1 column omitted\u001b[0m\n",
       "_.per_observation = [[[24800.0, 29400.0, ..., 5360.0], [4300.0, 31900.0, ..., 12600.0], [22400.0, 51600.0, ..., 35700.0], [1940.0, 22200.0, ..., 1920.0], [8920.0, 17900.0, ..., 6750.0]], [[6.15e8, 8.67e8, ..., 2.88e7], [1.85e7, 1.02e9, ..., 1.59e8], [5.03e8, 2.66e9, ..., 1.27e9], [3.76e6, 4.91e8, ..., 3.7e6], [7.96e7, 3.19e8, ..., 4.55e7]]]\n",
       "_.fitted_params_per_fold = [ … ]\n",
       "_.report_per_fold = [ … ]\n",
       "_.train_test_rows = [ … ]\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate!(mach,\n",
    "          measures=[l1, l2],\n",
    "          resampling=CV(nfolds=5, rng=123),\n",
    "          acceleration=CPUThreads())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.3",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 3
}
